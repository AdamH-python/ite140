{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arlington 2050 by Adam Hellinga\n",
    "\n",
    "The Arlington 2050 project was run by Arlington County, where residents could send in postcards from the future (the year 2050) talking about what improvements have been made to Arlington in that time. For this project, we were tasked with analyzing this data through numbers and visual representations of the data.\n",
    "\n",
    "All of this code was either written by me, Mr. Jones our teacher, or fellow students who researched these topics in depth to give an expert view on each of these topics.\n",
    "\n",
    "First, to get this all running, I need to import some modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from spellchecker import SpellChecker\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Cleaning the Data\n",
    "\n",
    "The data was split into a few excel files based on the collection point, and I analyzed the web input post cards, meaning this were submitted electronically and they were able to send up to 3 post cards as well as writing about how Arlington achieved these goals, however this section was misinterpreted frequently.\n",
    "\n",
    "Here I input the data set I will be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Public_Input_Postcards.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names were confusing, so here I rename the columns to make them easier to understand and easier to write out in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['id', 'zip', 'source', 'Card1', 'first_gettinghere','Card2','Card3','zip_selfreported','zip_selfreported2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to properly get word clouds, vectors, and all other aspects of this project all the postcards (one to three cards) needed to be condensed into one column, so here I went through each column with text and checked if it had a value, and if it did it was added to a column that is all the postcards in one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df.Card1)):\n",
    "    if pd.notna(df.loc[i][\"Card2\"]):\n",
    "        if pd.notna(df.loc[i][\"Card3\"]):\n",
    "            df.loc[i, \"Cards\"] = str(df.loc[i, \"Card1\"]) + \" \" + str(df.loc[i, \"Card2\"]) + \" \" + str(df.loc[i, \"Card3\"])\n",
    "        else:\n",
    "            df.loc[i, \"Cards\"] = str(df.loc[i, \"Card1\"]) + \" \" + str(df.loc[i, \"Card2\"])\n",
    "    else:\n",
    "        df.loc[i, \"Cards\"] = str(df.loc[i, \"Card1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We needed both all the postcards in one, but we also wanted all of the data in one column, so here they are combined into one. I used the cards column as it is already three of the four columns combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df.Card1)):\n",
    "    if pd.notna(df.loc[i][\"first_gettinghere\"]):\n",
    "        df.loc[i, \"All_text\"] = str(df.loc[i, \"Cards\"]) + \" \" + str(df.loc[i, \"first_gettinghere\"])\n",
    "    else:\n",
    "        df.loc[i, \"All_text\"] = str(df.loc[i, \"Cards\"])\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it into a Word Cloud\n",
    "This is done before the removal of stop words, as its possible that some important words may be lost in the spell check process.\n",
    "\n",
    "This word cloud shows how some of the important things that people want to see in 2050 are things like schools, parks, housing, places to walk, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100, mask=None, contour_width=3, contour_color='steelblue').generate(\" \".join(df.Cards))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "Here we analyze the spolarity, or how positive(1) or negative(-1) a statement is. We also analyze the subjectivity, or how objective(0) or subjective(1) a statement is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates two new columns, each with their respective value in them for each statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df.Cards)):\n",
    "    doc = nlp(df.loc[i][\"Cards\"])\n",
    "    df.loc[i, \"polarity\"] = doc._.blob.polarity\n",
    "    df.loc[i, \"subjectivity\"] = doc._.blob.subjectivity\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot the polarity of each statement on a graph, so we can see what the most common polarity is. This graph shows that most of these statements are slightly positive or neutral, and the rest are mostly positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x=df[\"polarity\"])\n",
    "plt.title('Polarity of Arlington 2050 Public Input Post Cards (Web)')\n",
    "plt.xlabel('polarity')\n",
    "plt.ylabel('count')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do the same thing, but we're graphing the Subjectivity. Here we can see that all the statements are usually a mix of objective and subjective statements, or they are entirely objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x=df[\"subjectivity\"])\n",
    "plt.title('Subjectivity of Arlington 2050 Public Input Post Cards (Web)')\n",
    "plt.xlabel('subjectivity')\n",
    "plt.ylabel('count')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we put in all the words that the spell checker doesn't automatically know, but we want it to remember and not delete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()\n",
    "\n",
    "spell.word_frequency.load_words([\n",
    "    'Arlington'\n",
    "    , 'Glebe'\n",
    "    , 'Ballston'\n",
    "    , 'Rosslyn'\n",
    "    , 'Pershing'\n",
    "    , 'Rockville'\n",
    "    , 'MD'\n",
    "    , 'VA'\n",
    "    , 'Maryland'\n",
    "    , 'Virginia'\n",
    "    , 'Bluemont'\n",
    "    , 'Wilson'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code goes through and finds which words are mispelled and corrects them, while also removing unnecessary words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check(text):\n",
    "    doc = nlp(text)  # Process the text with spaCy\n",
    "    corrected_words = []\n",
    "    \n",
    "    # Find misspelled words\n",
    "    misspelled = spell.unknown([token.text for token in doc if not token.is_punct and not token.is_stop])\n",
    "\n",
    "    for token in doc:\n",
    "        if not token.is_punct and not token.is_stop and len(token.text.strip()) > 0:  # Exclude punctuation and stop words\n",
    "            word = token.text.strip()\n",
    "            if word.lower() in misspelled:\n",
    "                correction = spell.correction(word)\n",
    "                if (correction is not None) and (correction.lower() != word.lower()):\n",
    "                    corrected_words.append(correction)\n",
    "                    #Uncomment this line to review the list of words that are correcting\n",
    "                    #print(f\"Correcting {word} => {correction}\")\n",
    "                else:\n",
    "                    corrected_words.append(word.lower())\n",
    "            else:\n",
    "                corrected_words.append(word)  # Preserve correct words\n",
    "\n",
    "    if len(corrected_words)>0:\n",
    "        return \" \".join(corrected_words)\n",
    "    else: \n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This puts the checked words into a new column called checked_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['checked_text'] = df['All_text'].apply(spell_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors\n",
    "Here we need to import the medium package instead of the small, as we need the vectors that are only found in the medium and large packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we put the topic we want it to look for, as well as how many of the most closely related values we want it to give."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"school\"\n",
    "related_values = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This finds how similar the given cell is to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarityToQuery(text):\n",
    "    return nlp(text).similarity(nlp(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This goes through all of the cells in the table and find their similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"similar_to_query\"] = df[\"checked_text\"].apply(similarityToQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loops through as many times as given to display the most closely related sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(related_values):\n",
    "    print(df.sort_values('similar_to_query', ascending=False).iloc[i][\"checked_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Finally, a summary that reflects on this project, what you've learned from it, and what you thought of it. Feel free to discuss whatever you want in this section, you might want to discuss Pandas, Spacy, Arlington, surveys, and/or data science.\n",
    "\n",
    "\n",
    "I believe this project was a great way to truly see what bring this community together, and I think that this data visualization was extremely helpful in identifying the themes and giving valuable visualizations of the data. I learned how to create word clouds, how to properly sort through data to remove empty cells and correct misspelled words, and I learned about polarity and subjectivity analysis. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
